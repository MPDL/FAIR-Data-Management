[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This material was created for the LMU & MPG Open Science Summer School 2024. During the Summer School the material was accompanied by slides and the tasks were done in groups. Independent of this event, the materials can be used to get an insight into Research Data Management in the context of Open Science and therefore with the goal of a FAIR publication of research data.\nThe materials can be used, remixed, adapted and shared according to the CCBY4.0 Licence."
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "FAIR Data Management Workshop",
    "section": "",
    "text": "Fragebögen MPDL\n\nHorizon Europe\nKurzer Fragenkatalog\nDFG-Checkliste\nVW-Stiftung – Sciene Europe\nRDMO\nFragenkatalog zur Erstellung einer Projekt-Forschungsdaten-Policy\nSoftware-Management-Plan für Forschende\nEuropean Research Council (ERC)\nDFG\nNFDI4Ing\nMath+ (Beta-Testmodus)\nMPIfG Project Management Plan (Beta-Testmodus)\nDatenmanagement für Klimamodellierung (Beta-Testmodus)\n\n\n\nFragebögen LMU\n\nRDMO\nDFG-Anträge (Alte Kulturen, Fachkollegium 101)\nDFG-Anträge (Chemie)\nDFG-Anträge (Physik)\nDFG-Anträge (Sozial- & Kulturanthropologie, Judaistik, Religionswissenschaft)\nDFG-Anträge (Wirtschafts/Sozialwissenschaften)\nDFG-Anträge (Wissenschaftliche Editionen in den Literaturwissenschaften)\nEuropean Research Council (ERC)\nHorizon 2020 Katalog\nHorizon Europe\nVW-Stiftung - Science Europe Datenmanagementplan\nSoftware-Management-Plan für Forschende\nDatenmanagement-Plan für Teilprojekte im SFB 1369"
  },
  {
    "objectID": "example-datasets.html",
    "href": "example-datasets.html",
    "title": "FAIR Data Management Workshop",
    "section": "",

    "text": "Example Datasets\nIntro Your job is to investigate an existing dataset (discuss and note what is good or bad about it, which alternatives exist), to improve it, and to ‘publish’ the improved version. Note: As you shall not really publish the improved dataset in the end, the discussion is more important than the result. Do not hesitate to contact us for questions and for involving us in the discussion. - Choose your dataset and download the data: - Dataset 1 (Groups 1, 2): https://doi.org/10.17617/3.1STIJV * Dataset 2 (Groups 3, 4): https://doi.org/10.5282/ubm/data.288 * Dataset 3 (Groups 5, 6): https://osf.io/6p9bf/ * Dataset 4 (Groups 7, 8): https://doi.org/10.5281/zenodo.10650332\n\nhttps://edmond.mpg.de/dataset.xhtml?persistentId=doi:10.17617/3.1STIJV\nhttps://data.ub.uni-muenchen.de/288/\nhttps://osf.io/6p9bf/\nhttps://zenodo.org/records/10650333"

  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "2) Documentation",
    "section": "",
    "text": "Effective documentation is a crucial aspect of FAIR data management, ensuring that research data is not only well-organized but also easily discoverable, accessible, and reusable by others. In this section, we’ll delve into the importance of documentation by exploring how to create best practice documentation that supports the entire research lifecycle. We’ll cover the essential components like including metadata that provides context and description of the data set, as well as README files that offer a concise introduction to the data set. Additionally, we’ll discuss the role of code books for making all components of the data set self-explanatory."
  },
  {
    "objectID": "documentation.html#overview",
    "href": "documentation.html#overview",
    "title": "2) Documentation",
    "section": "",
    "text": "Effective documentation is a crucial aspect of FAIR data management, ensuring that research data is not only well-organized but also easily discoverable, accessible, and reusable by others. In this section, we’ll delve into the importance of documentation by exploring how to create best practice documentation that supports the entire research lifecycle. We’ll cover the essential components like including metadata that provides context and description of the data set, as well as README files that offer a concise introduction to the data set. Additionally, we’ll discuss the role of code books for making all components of the data set self-explanatory.",
    "crumbs": [
      "Welcome",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#code-book",
    "href": "documentation.html#code-book",
    "title": "2) Documentation",
    "section": "Code book",

    "text": "Code book\nWhereas general metadata and README information typically focuses on the data set or project as a whole, code books delve deeper, providing detailed explanations and clarifications about the specific data contained in individual files. A code book should make all components of a data set file self-explanatory. It helps to identify e.g what certain variables and value labels mean or in which format the data points have been collected. This makes the data set more accessible by allowing re-users to work independent of a research paper.Some people include this information in the README file. However, we advise against it to keep the README concise and maintain its readability.\nIf all of your data files are structured and labeled according to the same system it is enough to provide one code book that describes and explain this system. If you have differently structured files you should provide multiple code books.\nHere are some of the elements that can be included in a code book:\n\nvariables\nvalue labels\ndata types\ndata format\ndata dictionary\ndata quality notes\ntechnical info\n\n\nBasic level:\nA first step is to provide this information about your data in a .csv table, a .txt file or .md (markdown) file. These are easy to create via a text editor of your choice. They can be opened by everyone independent of proprietary systems and are easily readable by humans.\n\n\n\n\n\n\nExample of a simple .csv code book\n\n\n\n\n\n“variable”,“data_type”,“format”\n“species”,“string”,“Adelie,Chinstrap,Gentoo”\n“population_size”,“integer”,“0-100000”\n“location”,“string”,“Latitude, Longitude”\n“date”,“string”,“YYYY-MM-DD”\n“latitude”,“number”,“-90 to 90”\n“longitude”,“number”,“-180 to 180”\n\n\n\n\n\n\nAdvanced level:\nTo improve the quality and usability of your code book you can provide it as a JSON file. This provides the advantage to be machine-readable and -actionable. It makes it easier to extract this information and work with it in coding scripts.\n\n\n\n\n\n\nExample of a detailed JSON code book\n\n\n\n\n\n{\n  \"title\": \"Penguin Populations Code book\",\n  \"description\": \"Code book for the Penguin Populations data set\",\n  \"variables\": [\n    {\n      \"name\": \"species\",\n      \"description\": \"The species of penguin\",\n      \"data_type\": \"string\",\n      \"format\": [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n    },\n    {\n      \"name\": \"population_size\",\n      \"description\": \"The number of penguins in the population\",\n      \"data_type\": \"integer\",\n      \"format\": \"0-100000\"\n    },\n    {\n      \"name\": \"location\",\n      \"description\": \"The region or location where the population is found\",\n      \"data_type\": \"string\",\n      \"format\": \"Latitude, Longitude\"\n    },\n    {\n      \"name\": \"date\",\n      \"description\": \"The date when the population size was recorded\",\n      \"data_type\": \"string\",\n      \"format\": \"YYYY-MM-DD\"\n    },\n    {\n      \"name\": \"latitude\",\n      \"description\": \"The latitude of the location\",\n      \"data_type\": \"number\",\n      \"format\": \"-90 to 90\"\n    },\n    {\n      \"name\": \"longitude\",\n      \"description\": \"The longitude of the location\",\n      \"data_type\": \"number\",\n      \"format\": \"-180 to 180\"\n    }\n  ],\n  \"data_dictionary\": {\n    \"species\": {\n      \"Adelie\": \"A small to medium-sized penguin species found in Antarctica\",\n      \"Chinstrap\": \"A medium-sized penguin species found in the Antarctic and sub-Antarctic\",\n      \"Gentoo\": \"A large penguin species found in the Antarctic and sub-Antarctic\"\n    }\n  },\n  \"data_quality_notes\": [\n    \"Population sizes are estimates and may not reflect the actual number of penguins in the population.\",\n    \"Locations are approximate and may not reflect the exact location of the population.\",\n    \"Dates are in the format YYYY-MM-DD and are in the UTC timezone.\"\n  ],\n  \"technical_info\": {\n    \"format\": \"JSON\",\n    \"compression\": \"gzip\",\n    \"required_software\": \"JSON parser\"\n  }\n}\n\n\n\n\n\n\n\n\n\n\nTask Z:\n\n\n\nOption 1: Your (example) data set does not contain a code book\n\nOpen one of the data files of your data set.\nCan you make sense of the data provided? Note down difficulties you identify.\nCan you find supportive information to understand the provided data in other documents? Note them down and link them if possible.\n\nOption 2: Your (example) data set does contain a code book\n\nIn what format was the code book provided?\nOpen one of the data files of your data set.\nCan you make sense of the data provided? Does the code book help you?"

  },
  {
    "objectID": "documentation.html#metadata",
    "href": "documentation.html#metadata",
    "title": "2) Documentation",
    "section": "Metadata",
    "text": "Metadata\nMetadata is “data that provides information about other data”1, describing the context, content, and structure of a dataset. It contains essential information about the data, such as authorship, creation context, contents, provenance and accessibility. Making it possible to understand and work with the data effectively. By providing a clear understanding of the data’s origin, and meaning, metadata plays a critical role in ensuring data quality, reproducibility, and FAIRness.\nMetadata should provide answers to the following questions:\n\nWho created the data set?\nWhat do the data files contain?\nWhen was the data set generated?\nWhere was the data set generated?\nWhy was the data set generated?\nHow was the data set generated?\nWhat research is this data set connected to?\nCan this data set be re-used?\n\n\n\n\n\n\n\nTask X:\n\n\n\nCan you answer all of these questions by looking through your (example) data set?\nWhere did you find this information?\nWas everything readily understandable?\nBonus:\nTime yourself - how long does it take you to answer all questions?\n\n\n\n\n\n\n\n\nExpand to learn more about where to find metadata\n\n\n\n\n\nMetadata can be found at several different points:\n\nRepository description: You are often asked about entering metadata, when you upload your data set to a repository. The quality and extent of metadata you can provide varies from repository to repository. We will cover repositories in Chapter 3). Jump here to go to the chapter directly.\nREADME file: Before you publish your data set you might already want to keep track of your metadata. A good way to not loose the overview yourself and inform others is by creating a README file. We will cover README files in the next section.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCommunity standards for metadata allow for improved comparison between data sets. Some disciplines have agreed-upon formal standards that define how metadata should be documented. If you publish your data via a discipline specific repository it is likely that their form already guides you through the community metadata standard. You can look for metadata standards that are specific to your discipline e.g. via FAIRsharing.org. If a standard for your research field exist you should follow it as best as possible.\nEven if your discipline does not adhere to strict standards, there is an advantage in using generic standards. By using controlled vocabularies or ontologies for your metadata you can increase the machine-readability and -actionability of your data.\nMachine-readability refers to the ability of machines (computers, algorithms, etc.) to automatically understand and interpret the meaning of metadata, such as keywords, descriptions, and concepts, without human intervention. Machine-actionability takes it a step further, enabling machines to not only understand the metadata but also to perform actions or take decisions based on that metadata, such as data integration, filtering, or visualization, without human intervention."
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "2) Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n„metadata“. Merriam-Webster Dictionary, accessed 7. Juli 2024, www.merriam-webster.com/dictionary/metadata.↩︎",
    "crumbs": [
      "Welcome",
      "2) Documentation"
    ]
  },
  {
    "objectID": "documentation.html#further-resources",
    "href": "documentation.html#further-resources",
    "title": "2) Documentation",
    "section": "Further Resources:",
    "text": "Further Resources:\n\n“Documentation and Metadata”, The Turing Way, accessed 7. July 2024. https://book.the-turing-way.org/reproducible-research/rdm/rdm-metadata"
  },
  {
    "objectID": "documentation.html#readme-file",
    "href": "documentation.html#readme-file",
    "title": "2) Documentation",
    "section": "README file",
    "text": "README file\nWe already established that metadata and README files are strongly connected. They form a great way to provide an introduction to and documentation about a data set and its research context. A README file is a plain text file often written in markdown and therefore named “README.md”. This naming convention helps spotting the README file for humans and machines alike, e.g. in GitHub repositories the README.md file is identified automatically and its content displayed on the repositories main page. The README file should be one of the first files your create for a new project. You can then continue filling it with the relevant information.\nWhat should be in a README file? - Ideally all the answers to the questions posed under the metadata section should be given in the README file. To determine what should be best included in your README file it can also be helpful to go through the questions of a Research Data Management Plan. While your README file should be very descriptive it is also important to stay concise and clear. Avoid e.g. using jargon to improve inter-disciplinary usage.\n\n\n\n\n\n\nTask Y:\n\n\n\nOption 1: Your (example) data set does not contain a README file\n\nCreate a README file using the template below in a text editor of your choice.\nFill out as much information as possible according to the metadata provided.\nMake note of the information that was difficult to obtain.\nUpload the file to your group folder.\n\nOption 2: Your (example) data set does contain a README file\n\nDownload and open the README file in a text editor of your choice.\nWhere does the provided information differ from the template below? Adapt the README file to improve its quality.\nMake note of any information missing from the template that you think improved the quality of the README.\nUpload your file to your group folder.\n\nExclude the last section “METHODOLOGICAL INFORMATION” for this task.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis readme file was generated on [YYYY-MM-DD] by [NAME]\n<help text in angle brackets should be deleted before finalizing your document> <[text in square brackets should be changed for your specific data set]>\n# GENERAL INFORMATION\nTitle of Data set: Description: <provide a short description of the study/project>\n<provide at least one contact>\nAuthor/Principal Investigator Information\nName:\nORCID:\nInstitution:\nEmail:\n\nDate of data collection: <provide single date, range, or approximate date; suggested format YYYY-MM-DD>\nGeographic location of data collection: <provide latitude, longitude, or city/region, State, Country>\nInformation about funding sources that supported the collection of the data:\n# SHARING/ACCESS Data\nLicenses/restrictions placed on the data:\nLinks to publications that cite or use the data:\nLinks to other publicly accessible locations of the data:\nLinks/relationships to ancillary data sets:\nWas data derived from another source? If yes, list source(s):\n# DATA & FILE OVERVIEW\nFile List: <list all files (or folders, as appropriate for data set organization) contained in the data set, with a brief description>\nRelationship between files, if important:\nAdditional related data collected that was not included in the current data package:\n# METHODOLOGICAL INFORMATION\nDescription of methods used for collection/generation of data: \nMethods for processing the data: \nInstrument- or software-specific information needed to interpret the data: <include full name and version of software, and any necessary packages or libraries needed to run scripts>\nStandards and calibration information, if appropriate:\nEnvironmental/experimental conditions:\nDescribe any quality-assurance procedures performed on the data:\nPeople involved with sample collection, processing, analysis and/or submission:\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor your own research project you can, of course, adapt this template to include the information that is relevant to your own data set. Also look out for other projects in your discipline and what they include in their README files.\nYou might also suggest to your department/research group/institute/research community to set up a discipline specific README template that can improve the interoperability, reusability and comparability of your research projects."
  },
  {
    "objectID": "documentation.html#basic-level",
    "href": "documentation.html#basic-level",
    "title": "2) Documentation",
    "section": "Basic level:",
    "text": "Basic level:\nA first step is to provide this information about your data in a .csv table, a .txt file or .md (markdown) file. These are easy to create via a text editor of your choice. They can also be opened by everyone independent of proprietary systems and easy to read for humans.\n\n\n\n\n\n\nExample of a simple .csv codebook\n\n\n\n“variable”,“data_type”,“format”\n“species”,“string”,“Adelie,Chinstrap,Gentoo”\n“population_size”,“integer”,“0-100000”\n“location”,“string”,“Latitude, Longitude”\n“date”,“string”,“YYYY-MM-DD”\n“latitude”,“number”,“-90 to 90”\n“longitude”,“number”,“-180 to 180”\n\n\n\nAdvanced: JSON –&gt; machine readable\nCan you convert a table into JSON try out on a plattform of your liking",
    "crumbs": [
      "Welcome",
      "2) Documentation"
    ]
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",

    "title": "3) Publication",
    "section": "",
    "text": "Data publication is the final step of FAIR data management, ensuring data accessibility, and when done well, also findability. If you have taken care of the previous sections and thereby made your data interoperable and reusable, publishing those data will take minimal effort. Most commonly, data are published as supplements to journal articles and an increasing number of journals actually requires that. Data sets can also be published in specialized data journals (e.g., Scientific data, Data in Brief), which means that the article itself contains a rich and detailed description of the data set. As you may have guessed, this option is mainly chosen for rather large, sampling-intensive data sets. Last but not least, it is also possible to publish an independent (without connected publication) data set in a repository and this is often required by funders as the DFG or EU. Irrespective of the publication option, there are common good practices when publishing data, namely defining usage conditions, ensuring reliable access, and choosing a repository, and these will be handled in the course of this chapter.\n\n\n\n\n\n\nLegal basis\n\n\n\nEven though publishing data (and code) is a prerequisite for transparent and reproducible research do note that in some cases the legal basis may prevent you from doing so. This most commonly concerns copyright or data protection issues. E.g., someone else may have the copyrights on the data you are working with, or when working with personal data (i.e., data from identifiable living people), you would violate subjects’ rights according to the general data protection regulations (GDPR). There is a very nice decision tree (unfortunately only in German) that can help you find out whether and under what conditions you can publish your data set. Broadly speaking, publishing personal data is only possible if either they can be fully anonymized (and no longer fall under GDPR) or if participants consented to their data being published. It is hence important to consider even before collecting personal data, whether they ought to be published to eventually include it in the consent form."

  },
  {
    "objectID": "publication.html#data-andor-code-availability-statement",
    "href": "publication.html#data-andor-code-availability-statement",

    "title": "3) Publication",
    "section": "Data and/or Code Availability Statement",
    "text": "Data and/or Code Availability Statement\nA Data Availability Statement (also sometimes called a ‘data access statement’) tells the reader where the research data associated with a paper is available, and under what conditions the data can be accessed. They also include links (where applicable) to the data set.\nData Availability Statements should generally be placed in the back matter of your manuscript, just before your References, unless your chosen journal specifies otherwise.\nHowever, if your journal uses a double-blind peer review process, it’s important to ensure that the information in your Data Availability Statement doesn’t compromise the anonymity of you or your co-authors. If there is information in your Data Availability Statement that could be used to identify the manuscript authors, please submit your Data Availability Statement the same way as you would any other identifying information associated with your manuscript, as instructed by your journal. This might be for example as part of a Title Page, submitted as a separate document from your manuscript.\nSource: https://www.cambridge.org/core/services/authors/open-data/data-availability-statements\n\n\n\n\n\n\nTask X:\n\n\n\nIs your data set the basis for a published article?\nIf so, does the article include a Data Availability Statement?"

  },
  {
    "objectID": "publication.html#license",
    "href": "publication.html#license",

    "title": "3) Publication",
    "section": "License",
    "text": "License\nIf we in the end would like to share our research data and enable reuse for other researchers, we should keep in mind that our data might be protected by copyright law. This depends on the level of originality in the data and here it is very important to differentiate between the instrument and the data. If you developed a comprehensive questionnaire or an elaborate fMRI task then this is the thing that ought to be protected whereas the data are most likely not. In any case, you should define conditions under which the data can be reused. And this is actually where licenses come into play. Licenses are in a way standard contracts that regulate usage rights for published work. And as I already mentioned, thereby enables other people to reuse the published work. If no license is provided with your dataset, it is not clear to others under which conditions they can reuse it and they might (against your good intentions) rather refrain from reusing it at all. There are several licenses out there, as e.g. Creative Commons, MIT (software), Apache (software), Open Data Commons. Rules of thumb: Go with the least restrictive license! CC0 if unsure of copyright CC-BY if holding and insisting on copyright Public Domain Mark if not protected by copyright\n\n\n\n\n\n\nTask X:\n\n\n\nAre you actually allowed to reuse your data set?\nIf so, under which conditions?\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom our point of view the creative commons licenses are somewhat dominating in the area of research and we will therefore go into a little bit of detail on these. CC provides four features, which form the basis of a fixed set of six CC licenses. These are attribution (the obligation to credit the author and other parties designated for attribution); “ND” means NoDerivatives (only verbatim copies of the work can be shared); “NC” stands for NonCommercial (commercial use is excluded from the license grant and “SA” represents ShareAlike (i.e. the work can be modified and modified versions can be published but only under the original or a compatible license). And these are the 6 licenses that result from the combination of the four features. The least restrictive is the CC BY license: It grants an unrestricted license to use the respective content. How the content is used, e.g. in original or modified form, by whom or for what purpose, is irrelevant. In addition, there are 2 further labels, to dedicate the published work to the public domain, these are CC0 and the public domain mark. When you put your work under CC0 you are allowing others to reuse your work without conditions, so even if you were copyright holder you are not insisting on it. I do want to stress at this point that the good scientific practice obviously foresee that you will in any case credit the author of the work, so in a way, this would be the preferred option. And then there is the Public domain mark which can be used to label work that is not restricted by copyright.\nTo help you decide on a licence, the is Choo-choo-choose your license."

  },
  {
    "objectID": "publication.html#persistent-identifier",
    "href": "publication.html#persistent-identifier",

    "title": "3) Publication",
    "section": "Persistent identifier",
    "text": "Persistent identifier\nImagine a scenario in which a repository is no longer maintained, so that all data sets published in that repository are no longer available at the repository’s URLs (a quite real scenario, as you can see in this article. To avoid data loss, the original repository migrated its data sets to another repository, where it however gets a new URL. Consequently, if the corresponding article linked to its data set via the URL (e.g., https://osf.io/gn47c/), it can no longer be found. What now? In such cases, persistent identifiers are extremely helpful if not essential. The ones we consider crucial are listed below.\n\nDigital object identifier\nA digital object identifier (DOI) points directly to a digital object rather than to its location online (unlike URL). You may know it already from publications. It consists of a unique number made up of a prefix and a suffix separated by a forward slash as e.g., 10.5281/zenodo.4322849. It allows things to be uniquely identified and accessed reliably by using the DOI proxy server https://doi.org/ and appending the DOI, e.g., https://doi.org/10.5281/zenodo.4322849.\n\n\nORCID\n(Open Researcher and Contributor ID) https://orcid.org/ Unique scientific ID, even when changing name/ institution etc.Persistent even if: • Changing name/ institution etc. • Spelling errors\n\n\nROR\n(Research Organization Registry) Unique identifier for every research organization in the world https://ror.org/\n\n\n\n\n\n\nTask X:\n\n\n\nCan your dataset be accessed reliably?\nWho would you contact, if questions concering the data set arose?\nIs that contact still available?"

  },
  {
    "objectID": "publication.html#repository",
    "href": "publication.html#repository",

    "title": "3) Publication",
    "section": "Repository",
    "text": "Repository\nRepositories are database systems to document, store and publish digital objects. Thereby, they ensure visibility (via meta data) and sustainability (via use of PIDs) of the digital objects they are holding. Repositories can be: * Discipline-specific (e.g. OpenNeuro) * General (e.g. Zenodo, OSF) * Institutional (e.g. Edmond MPG, LMU)\nThe Registry of research data repositories (Re3data.org) provides a collection of repositories with very nice search and overview functions. E.g., you can browse by subject (i.e., discipline).  When looking for a repository there are various criteria that should be considered.\n\nCriteria\n\nCosts/ User rights\nHosting country (GDPR)\nPersistent identifier\nLicense\nAccess control/ Embargo\nCertified/ Sustainability\nData limit\n\nWhen searching re3data, most of these criteria are displayed beside the repository, so can immediately check whether a potential repository fulfills your requirements.\n\n\n\nre3data\n\n\n\n\nFAIR assessment tools https://zenodo.org/records/7701941\n\nShort questionnaire\nF-uji Tool"

  }
]